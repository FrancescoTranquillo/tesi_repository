giorni_predittivi <- sapply(giorni_guasti, backprop)
# assegno flag=1 in corrispondenza dei giorni predittivi
df%<>%mutate(flag=ifelse(as.Date(df$GIORNO)%in%giorni_predittivi,1,0))
clean_corpus <- function(corpus) {
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
# corpus <- tm_map(corpus, stemDocument)
}
meta <- function(bag) {
bag_text <- bag$INSTANCES$testo
bag_corpus <- VCorpus(VectorSource(bag_text)) %>%
clean_corpus(.)
bag_dtm <- as.data.frame(as.matrix(DocumentTermMatrix(
bag_corpus,
control = list(
dictionary = testo_dict
)
)))
tfidf <- summarise_all(bag_dtm, mean, na.rm = T)
cbind("TARGET" = bag$BAG_FLAG, tfidf)
}
df$testo <- iconv(df$testo,"UTF-8", "UTF-8",sub='')
testo_corpus <- VCorpus(VectorSource(df$testo))
testo_corpus_clean<-clean_corpus(testo_corpus)
testo_dtm<- DocumentTermMatrix(testo_corpus_clean)
testo_dict <- findFreqTerms(testo_dtm, lowfreq = 50)
trainIndex <- createDataPartition(df$flag, p = .95,
list = FALSE,
times = 1)
df_training <- df[trainIndex,]
df_test <- df[-trainIndex,]
df <- df_training
df_pos_test <- df[which(df_test$flag==1),] %>%
mutate("BAG"=factor(cut.Date(.$GIORNO, breaks = "1 days",labels = F)))
df_pos_test_bag <- as.list(split(df_pos_test,f = df_pos_test$BAG))
#le righe con flag=1 sono le bag positive
#bag "negative", separate in bags da 5 giorni
df_neg_test <- df[-which(df_test$flag==1),] %>%
mutate("BAG"=factor(cut.Date(.$GIORNO, breaks = "1 days",labels = F)))
df_neg_test_bag <- as.list(split(df_neg_test,f = df_neg_test$BAG))
# assegno ad ogni elemento delle bag create il flag 1 o 0
df_pos_test_bag <- lapply(df_pos_test_bag,function(x) list("INSTANCES"=x,
"BAG_FLAG"=1))
df_neg_test_bag <- lapply(df_neg_test_bag,function(x) list("INSTANCES"=x,
"BAG_FLAG"=0))
bags_test <- c(df_pos_test_bag,df_neg_test_bag)
df_meta_test <- pblapply(bags_test,meta) %>%
do.call("rbind",.) %>%
.[sample(nrow(.)),]
df_meta_test$TARGET <-  factor(df_meta_test$TARGET)
levels(df_meta_test$TARGET) <- c("neg", "pos")
# La predizione viene fatta ogni giorno. Si predice se il pacchetto di scontrini della giornata appartiene
# al gruppo flag = 1 (scontrini 5 giorni prima di un guasto)
df_pos <- df[which(df$flag==1),] %>%
mutate("BAG"=factor(cut.Date(.$GIORNO, breaks = "7 days",labels = F)))
df_pos_bag <- as.list(split(df_pos,f = df_pos$BAG))
#le righe con flag=1 sono le bag positive
#bag "negative", separate in bags da 5 giorni
df_neg <- df[-which(df$flag==1),] %>%
mutate("BAG"=factor(cut.Date(.$GIORNO, breaks = "7 days",labels = F)))
df_neg_bag <- as.list(split(df_neg,f = df_neg$BAG))
# assegno ad ogni elemento delle bag create il flag 1 o 0
df_pos_bag <- lapply(df_pos_bag,function(x) list("INSTANCES"=x,
"BAG_FLAG"=1))
df_neg_bag <- lapply(df_neg_bag,function(x) list("INSTANCES"=x,
"BAG_FLAG"=0))
bags <- c(df_pos_bag,df_neg_bag)
# trasformo ogni bag
df_meta <- pblapply(bags,meta) %>%
do.call("rbind",.) %>%
.[sample(nrow(.)),]
# unify <- function(bag){
#   bag_text <- bag$INSTANCES$testo %>%
#     paste(.,collapse="|")
#   data.frame("testo"=bag_text,
#                 "flag"=bag$BAG_FLAG)
# }
#
# tm <- lapply(c(df_pos_bag,df_neg_bag), unify) %>%
#   do.call("rbind",.) %>%
#   .[sample(nrow(.)),]
trainIndex <- createDataPartition(df_meta$TARGET, p = 1,
list = FALSE,
times = 1)
tm_training <- df_meta[ trainIndex,]
tm_testing <-  df_meta[-trainIndex,]
tm_training$TARGET<- factor(tm_training$TARGET)
tm_testing$TARGET <-factor( tm_testing$TARGET)
levels(tm_training$TARGET) <- c("neg", "pos")
levels(tm_testing$TARGET) <- c("neg", "pos")
table(tm_training$TARGET)
table(df_meta_test$TARGET)
fitControl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 2,
verboseIter=T,
classProbs = TRUE,
allowParallel = T,
sampling = "down"
# summaryFunction = twoClassSummary
)
model_weights <- ifelse(tm_training$TARGET == "pos",
(1/table(tm_training$TARGET)[1]) * 0.5,
(1/table(tm_training$TARGET)[2]) * 0.5)
set.seed(1045)
# library(doParallel)
# cl <- makeCluster(16)
# registerDoParallel(cl)
fitControl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
verboseIter=T,
classProbs = TRUE,
allowParallel = T,
sampling = "down"
# summaryFunction = twoClassSummary
)
nn <-
train(TARGET~.,
data=tm_training,
method = 'nnet',
trControl = fitControl,
tuneLength=3,
# maxit=200,
preProcess=c("range")
# metric="Kappa"
# weights = model_weights
)
# stopCluster(cl)
predictions <- predict(nn,newdata =df_meta_test)
confusionMatrix(predictions,df_meta_test$TARGET, positive = "pos",mode = "everything")
library(mltools)
mcc(preds = predictions, df_meta_test$TARGET)
View(tm_training)
View(tm_training)
method = 'glm,
nn <-
train(TARGET~.,
data=tm_training,
method = 'glm',
trControl = fitControl,
tuneLength=3,
# maxit=200,
preProcess=c("range")
# metric="Kappa"
# weights = model_weights
)
nn <-
train(TARGET~.,
data=tm_training,
method = "glm",
trControl = fitControl,
tuneLength=3,
# maxit=200,
preProcess=c("range")
# metric="Kappa"
# weights = model_weights
)
# stopCluster(cl)
predictions <- predict(nn,newdata =df_meta_test)
confusionMatrix(predictions,df_meta_test$TARGET, positive = "pos",mode = "everything")
library(mltools)
mcc(preds = predictions, df_meta_test$TARGET)
nn <-
train(TARGET~.,
data=tm_training,
method = "bayes_glm",
trControl = fitControl,
tuneLength=3,
# maxit=200,
preProcess=c("range")
# metric="Kappa"
# weights = model_weights
)
nn <-
train(TARGET~.,
data=tm_training,
method = "bayesglm",
trControl = fitControl,
tuneLength=3,
# maxit=200,
preProcess=c("range")
# metric="Kappa"
# weights = model_weights
)
# stopCluster(cl)
predictions <- predict(nn,newdata =df_meta_test)
confusionMatrix(predictions,df_meta_test$TARGET, positive = "pos",mode = "everything")
library(mltools)
mcc(preds = predictions, df_meta_test$TARGET)
nn <-
train(TARGET~.,
data=tm_training,
method = "naive_bayes",
trControl = fitControl,
tuneLength=3,
# maxit=200,
preProcess=c("range")
# metric="Kappa"
# weights = model_weights
)
# stopCluster(cl)
predictions <- predict(nn,newdata =df_meta_test)
confusionMatrix(predictions,df_meta_test$TARGET, positive = "pos",mode = "everything")
library(mltools)
mcc(preds = predictions, df_meta_test$TARGET)
varImp(nn)
mcc(preds = predictions, df_meta_test$TARGET)
nn <-
train(TARGET~.,
data=tm_training,
method = "svmLinear3",
trControl = fitControl,
tuneLength=3,
# maxit=200,
preProcess=c("range")
# metric="Kappa"
# weights = model_weights
)
# stopCluster(cl)
predictions <- predict(nn,newdata =df_meta_test)
confusionMatrix(predictions,df_meta_test$TARGET, positive = "pos",mode = "everything")
library(mltools)
mcc(preds = predictions, df_meta_test$TARGET)
nn <-
train(TARGET~.,
data=tm_training,
method = "svmRadial",
trControl = fitControl,
tuneLength=3,
# maxit=200,
preProcess=c("range")
# metric="Kappa"
# weights = model_weights
)
# stopCluster(cl)
predictions <- predict(nn,newdata =df_meta_test)
confusionMatrix(predictions,df_meta_test$TARGET, positive = "pos",mode = "everything")
library(mltools)
mcc(preds = predictions, df_meta_test$TARGET)
varImp()
varImp(nn)
plot(nn)
nn <-
train(TARGET~.,
data=tm_training,
method = "pcaNNet",
trControl = fitControl,
tuneLength=3,
# maxit=200,
preProcess=c("range")
# metric="Kappa"
# weights = model_weights
)
# stopCluster(cl)
predictions <- predict(nn,newdata =df_meta_test)
confusionMatrix(predictions,df_meta_test$TARGET, positive = "pos",mode = "everything")
library(mltools)
mcc(preds = predictions, df_meta_test$TARGET)
# library(doParallel)
# cl <- makeCluster(16)
# registerDoParallel(cl)
fitControl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter=T,
classProbs = TRUE,
allowParallel = T,
sampling = "down"
# summaryFunction = twoClassSummary
)
nn <-
train(TARGET~.,
data=tm_training,
method = "nnet",
trControl = fitControl,
tuneLength=10,
# maxit=200,
preProcess=c("range")
# metric="Kappa"
# weights = model_weights
)
# stopCluster(cl)
predictions <- predict(nn,newdata =df_meta_test)
confusionMatrix(predictions,df_meta_test$TARGET, positive = "pos",mode = "everything")
mcc(preds = predictions, df_meta_test$TARGET)
library(magrittr)
library(rayshader)
library(av)
localtif = raster::raster(file.choose())
#And convert it to a matrix:
elmat = matrix(raster::extract(localtif, raster::extent(localtif), buffer = 1000),
nrow = ncol(localtif), ncol = nrow(localtif))
library(png)
# define label
zscale <- 0.5
#We use another one of rayshader's built-in textures:
#We use another one of rayshader's built-in textures:
elmat %>%
add_overlay(foto,alphacolor = "white",alphalayer = 0.9) %>%
# add_water(detect_water(elmat)) %>%
add_shadow(ambient_shade(elmat)) %>%
theta = -16, phi = 40, zoom = 0.7, fov = 10
#We use another one of rayshader's built-in textures:
elmat %>%
sphere_shade(texture = "imhof1") %>%
add_overlay(foto,alphacolor = "white",alphalayer = 0.9) %>%
add_shadow(ray_shade(elmat,lambert=FALSE)) %>%
# add_water(detect_water(elmat)) %>%
add_shadow(lamb_shade(elmat)) %>%
add_shadow(ambient_shade(elmat)) %>%
plot_3d(elmat, solid = T,zscale = zscale, windowsize = c(1000, 800),
theta = -16, phi = 40, zoom = 0.7, fov = 10)
#We use another one of rayshader's built-in textures:
elmat %>%
sphere_shade(texture = "imhof1") %>%
# add_overlay(foto,alphacolor = "white",alphalayer = 0.9) %>%
add_shadow(ray_shade(elmat,lambert=FALSE)) %>%
# add_water(detect_water(elmat)) %>%
add_shadow(lamb_shade(elmat)) %>%
add_shadow(ambient_shade(elmat)) %>%
plot_3d(elmat, solid = T,zscale = zscale, windowsize = c(1000, 800),
theta = -16, phi = 40, zoom = 0.7, fov = 10)
# define label
zscale <- 20
#We use another one of rayshader's built-in textures:
elmat %>%
sphere_shade(texture = "imhof1") %>%
# add_overlay(foto,alphacolor = "white",alphalayer = 0.9) %>%
add_shadow(ray_shade(elmat,lambert=FALSE)) %>%
# add_water(detect_water(elmat)) %>%
add_shadow(lamb_shade(elmat)) %>%
add_shadow(ambient_shade(elmat)) %>%
plot_3d(elmat, solid = T,zscale = zscale, windowsize = c(1000, 800),
theta = -16, phi = 40, zoom = 0.7, fov = 10)
localtif = raster::raster(file.choose())
#And convert it to a matrix:
elmat = matrix(raster::extract(localtif, raster::extent(localtif), buffer = 1000),
nrow = ncol(localtif), ncol = nrow(localtif))
library(png)
# define label
zscale <- 20
#We use another one of rayshader's built-in textures:
elmat %>%
sphere_shade(texture = "imhof1") %>%
# add_overlay(foto,alphacolor = "white",alphalayer = 0.9) %>%
add_shadow(ray_shade(elmat,lambert=FALSE)) %>%
# add_water(detect_water(elmat)) %>%
add_shadow(lamb_shade(elmat)) %>%
add_shadow(ambient_shade(elmat)) %>%
plot_3d(elmat, solid = T,zscale = zscale, windowsize = c(1000, 800),
theta = -16, phi = 40, zoom = 0.7, fov = 10)
# define label
zscale <- 3
#We use another one of rayshader's built-in textures:
elmat %>%
sphere_shade(texture = "imhof1") %>%
# add_overlay(foto,alphacolor = "white",alphalayer = 0.9) %>%
add_shadow(ray_shade(elmat,lambert=FALSE)) %>%
# add_water(detect_water(elmat)) %>%
add_shadow(lamb_shade(elmat)) %>%
add_shadow(ambient_shade(elmat)) %>%
plot_3d(elmat, solid = T,zscale = zscale, windowsize = c(1000, 800),
theta = -16, phi = 40, zoom = 0.7, fov = 10)
# define label
zscale <- 1
#We use another one of rayshader's built-in textures:
elmat %>%
sphere_shade(texture = "imhof1") %>%
# add_overlay(foto,alphacolor = "white",alphalayer = 0.9) %>%
add_shadow(ray_shade(elmat,lambert=FALSE)) %>%
# add_water(detect_water(elmat)) %>%
add_shadow(lamb_shade(elmat)) %>%
add_shadow(ambient_shade(elmat)) %>%
plot_3d(elmat, solid = T,zscale = zscale, windowsize = c(1000, 800),
theta = -16, phi = 40, zoom = 0.7, fov = 10,baseshape = "circle")
foto <- png::readPNG(file.choose())
#And convert it to a matrix:
elmat = matrix(raster::extract(localtif, raster::extent(localtif), buffer = 1000),
nrow = ncol(localtif), ncol = nrow(localtif))
# define label
zscale <- 1
#We use another one of rayshader's built-in textures:
elmat %>%
sphere_shade(texture = "imhof1") %>%
# add_overlay(foto,alphacolor = "white",alphalayer = 0.9) %>%
add_shadow(ray_shade(elmat,lambert=FALSE)) %>%
# add_water(detect_water(elmat)) %>%
add_shadow(lamb_shade(elmat)) %>%
add_shadow(ambient_shade(elmat)) %>%
plot_3d(elmat, solid = T,zscale = zscale, windowsize = c(1000, 800),
theta = -16, phi = 40, zoom = 0.7, fov = 10,baseshape = "circle")
# define label
zscale <-40
#We use another one of rayshader's built-in textures:
elmat %>%
sphere_shade(texture = "imhof1") %>%
# add_overlay(foto,alphacolor = "white",alphalayer = 0.9) %>%
add_shadow(ray_shade(elmat,lambert=FALSE)) %>%
# add_water(detect_water(elmat)) %>%
add_shadow(lamb_shade(elmat)) %>%
add_shadow(ambient_shade(elmat)) %>%
plot_3d(elmat, solid = T,zscale = zscale, windowsize = c(1000, 800),
theta = -16, phi = 40, zoom = 0.7, fov = 10,baseshape = "circle")
nn <-
train(TARGET~.,
data=tm_training,
method = "glm",
trControl = fitControl,
tuneLength=3,
# maxit=200,
preProcess=c("range")
# metric="Kappa"
# weights = model_weights
)
# stopCluster(cl)
predictions <- predict(nn,newdata =df_meta_test)
confusionMatrix(predictions,df_meta_test$TARGET, positive = "pos",mode = "everything")
nn <-
train(TARGET~.,
data=tm_training,
method = "gbm",
trControl = fitControl,
tuneLength=3,
# maxit=200,
preProcess=c("range")
# metric="Kappa"
# weights = model_weights
)
# stopCluster(cl)
predictions <- predict(nn,newdata =df_meta_test)
confusionMatrix(predictions,df_meta_test$TARGET, positive = "pos",mode = "everything")
library(mltools)
mcc(preds = predictions, df_meta_test$TARGET)
nn <-
train(TARGET~.,
data=tm_training,
method = "svmLinear",
trControl = fitControl,
tuneLength=3,
# maxit=200,
preProcess=c("range")
# metric="Kappa"
# weights = model_weights
)
# stopCluster(cl)
predictions <- predict(nn,newdata =df_meta_test)
confusionMatrix(predictions,df_meta_test$TARGET, positive = "pos",mode = "everything")
library(mltools)
mcc(preds = predictions, df_meta_test$TARGET)
df
<-
df <-
read.csv2(file = "tabella_scontrini_allarmi.csv",
header = T,
stringsAsFactors = F,encoding = "UTF-8")
head(df)
n <- ncol(df)
df[,c(2,4:(n-2))] %<>% lapply(function(x) factor(x))
n <- ncol(df)
df[,c(2,4:(n-2))] %<>% lapply(function(x) factor(x))
library(tidyverse)
library(dplyr)
library(lubridate)
library(here)
library(readxl)
library(magrittr)
library(DataExplorer)
library(caret)
library(rlist)
library(nnet)
library(NeuralNetTools)
library(esquisse)
library(tm)
library(pbapply)
library(pbapply)
# library(pROC)
rm(list=ls())
df <-
read.csv2(file = "tabella_scontrini_allarmi.csv",
header = T,
stringsAsFactors = F,encoding = "UTF-8")
n <- ncol(df)
df[,c(2,4:(n-2))] %<>% lapply(function(x) factor(x))
df$INIZIO.CICLO <-
parse_date_time(df$INIZIO.CICLO, orders = "dmy hms")
coswin <- read.csv2(file = "coswin-isa/108841.csv",
header = T,
stringsAsFactors = F) %>%
.[-which(grepl("inseri|verifica|ordinaria", x = .$Descrizione,ignore.case = T)),c(22,24)] %>%
.[which(complete.cases(.)),] %>%
unique(.)
coswin$Data.Richiesta <- dmy_hm(coswin$Data.Richiesta) %>%
as_date()
coswin$Data.Richiesta[2]
glimpse(coswin)
View(coswin)
shiny::runApp('ISA-test')
rds <- as.list(list.files(here::here(),"*mm_*"))
rds <- as.list(list.files(here::here(),"\*mm_*"))
rds <- as.list(list.files(here::here(),"\\*mm_*"))
rds <- as.list(list.files(here::here(),"mm_*"))
list.files(here::here(),"mm_*")
rds <- as.list(list.files(here::here(),"mm_*")[1:5])
runApp('ISA-test')
rds <- as.list(list.files(here(),"mm_*")[1:5])
here()
rds <- as.list(list.files(here::here("ISA-test"),"mm_*"))
rds
runApp('ISA-test')
